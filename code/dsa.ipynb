{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44947c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from io import StringIO\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import zipfile\n",
    "import time\n",
    "\n",
    "\n",
    "DATA_ROOT = Path(\"../data\")\n",
    "\n",
    "SECTION_TITLES = [\n",
    "    \"Accounts and Servers: Warnings & Temporary Interventions\",\n",
    "    \"Accounts Disabled\",\n",
    "    \"Servers Removed\",\n",
    "    \"Appeals\",\n",
    "    \"Reports\",\n",
    "    \"NCMEC\",\n",
    "    \"US Gov Info Requests\",\n",
    "    \"International Government Information Requests\",\n",
    "    \"Preservation Requests\",\n",
    "    \"Emergency Requests\"\n",
    "]\n",
    "\n",
    "MONTHS = {\n",
    "    \"jan\": 1, \"feb\": 2, \"mar\": 3, \"apr\": 4, \"may\": 5, \"jun\": 6,\n",
    "    \"jul\": 7, \"aug\": 8, \"sep\": 9, \"oct\": 10, \"nov\": 11, \"dec\": 12\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9536a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base URL of the DSA Transparency Portal\n",
    "base_url = \"https://transparency.dsa.ec.europa.eu/explore-data/download\"\n",
    "platform_id = 59  # Discord\n",
    "page = 1  # Start from the first page\n",
    "\n",
    "os.makedirs('../data/dsa_zip_files', exist_ok=True)\n",
    "os.makedirs('../data/dsa_extracted', exist_ok=True)\n",
    "\n",
    "def unzip_recursive(folder_path):\n",
    "    \"\"\"\n",
    "    Recursively unzip all zip files in a folder and its subfolders\n",
    "    \"\"\"\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".zip\"):\n",
    "                zip_path = os.path.join(root, file)\n",
    "                extract_folder = os.path.join(root, file.replace(\".zip\", \"\"))\n",
    "                if not os.path.exists(extract_folder):\n",
    "                    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "                        zip_ref.extractall(extract_folder)\n",
    "                    print(f\"Extracted {zip_path} -> {extract_folder}\")\n",
    "                    # After extracting, recursively unzip inside\n",
    "                    unzip_recursive(extract_folder)\n",
    "\n",
    "def concatenate_csvs(folder_path, output_filename):\n",
    "    \"\"\"\n",
    "    Concatenate all CSVs in a folder (and its subfolders) into a single CSV\n",
    "    \"\"\"\n",
    "    all_files = []\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".csv\"):\n",
    "                all_files.append(os.path.join(root, file))\n",
    "    if all_files:\n",
    "        df_list = [pd.read_csv(f) for f in all_files]\n",
    "        combined_df = pd.concat(df_list, ignore_index=True)\n",
    "        combined_df.to_csv(output_filename, index=False)\n",
    "        print(f\"Concatenated {len(all_files)} CSVs into {output_filename}\")\n",
    "    else:\n",
    "        print(f\"No CSV files found in {folder_path}\")\n",
    "\n",
    "while True:\n",
    "    print(f\"Processing page {page}...\")\n",
    "    params = {\"platform_id\": platform_id, \"page\": page}\n",
    "    response = requests.get(base_url, params=params)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Find all ZIP links in the \"full\" column\n",
    "    links = soup.find_all('a', href=True)\n",
    "    zip_links = [link['href'] for link in links if 'full' in link['href'] and link['href'].endswith('.zip')]\n",
    "    \n",
    "    if not zip_links:\n",
    "        print(\"No more ZIP files found. Finished downloading all pages.\")\n",
    "        break\n",
    "    \n",
    "    for zip_url in zip_links:\n",
    "        zip_filename = os.path.join('../data/dsa_zip_files', zip_url.split('/')[-1])\n",
    "        extract_folder = os.path.join('../data/dsa_extracted', zip_url.split('/')[-1].replace('.zip', ''))\n",
    "\n",
    "        # sanity check\n",
    "        if not os.path.exists(zip_filename):\n",
    "            print(f\"Downloading {zip_filename}...\")\n",
    "            zip_response = requests.get(zip_url)\n",
    "            zip_response.raise_for_status()\n",
    "            with open(zip_filename, 'wb') as f:\n",
    "                f.write(zip_response.content)\n",
    "            print(f\"Downloaded {zip_filename}\")\n",
    "        else:\n",
    "            print(f\"{zip_filename} already exists. Skipping download.\")\n",
    "        \n",
    "        if not os.path.exists(extract_folder):\n",
    "            with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
    "                zip_ref.extractall(extract_folder)\n",
    "            print(f\"Extracted to {extract_folder}\")\n",
    "        else:\n",
    "            print(f\"{extract_folder} already exists. Skipping extraction.\")\n",
    "        \n",
    "        # unzip nested zips\n",
    "        unzip_recursive(extract_folder)\n",
    "        \n",
    "        # concatenate data for each day (folder) in a new csv\n",
    "        output_csv = os.path.join('../data/dsa_extracted', zip_url.split('/')[-1].replace('.zip', '_combined.csv'))\n",
    "        if(not os.path.exists(output_csv)):\n",
    "            concatenate_csvs(extract_folder, output_csv)\n",
    "    \n",
    "    page += 1\n",
    "    time.sleep(1)\n",
    "\n",
    "# concatenate all the days into one big csv\n",
    "os.makedirs('../data/dsa_processed', exist_ok=True)\n",
    "\n",
    "print(\"Concatenating all combined CSVs into one big dataset...\")\n",
    "\n",
    "# Collect all *_combined.csv files created earlier\n",
    "daily_combined_files = [\n",
    "    os.path.join('../data/dsa_extracted', f)\n",
    "    for f in os.listdir('../data/dsa_extracted')\n",
    "    if f.endswith('_combined.csv')\n",
    "]\n",
    "\n",
    "if daily_combined_files:\n",
    "    df_list = [pd.read_csv(f, low_memory=False) for f in daily_combined_files]\n",
    "    all_data = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "    # Save to a single file (no date suffix)\n",
    "    final_output = '../data/dsa_processed/all_discord_data_combined.csv'\n",
    "    all_data.to_csv(final_output, index=False)\n",
    "    print(f\"All {len(daily_combined_files)} daily CSVs concatenated into {final_output}\")\n",
    "    print(f\"Final shape: {all_data.shape}\")\n",
    "else:\n",
    "    print(\"No per-day combined CSV files found in ../data/dsa_extracted/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4524b379",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = final_output\n",
    "\n",
    "COLUMNS = [\n",
    "    \"decision_account\",\n",
    "    \"decision_ground\",\n",
    "    \"decision_ground_reference_url\",\n",
    "    \"illegal_content_legal_ground\",\n",
    "    \"illegal_content_explanation\",\n",
    "    \"incompatible_content_ground\",\n",
    "    \"incompatible_content_explanation\",\n",
    "    \"incompatible_content_illegal\",\n",
    "    \"category\",\n",
    "    \"category_addition\",\n",
    "    \"category_specification\",\n",
    "    \"category_specification_other\",\n",
    "    \"content_type\",\n",
    "    \"content_type_other\",\n",
    "    \"content_date\",\n",
    "    \"application_date\",\n",
    "    \"source_type\",\n",
    "    \"automated_detection\",\n",
    "    \"automated_decision\",\n",
    "    \"created_at\"\n",
    "]\n",
    "\n",
    "df = pd.read_csv(\n",
    "    path,\n",
    "    usecols=lambda c: c in COLUMNS,\n",
    "    dtype=str,          # treat all as strings to speed up parsing\n",
    "    low_memory=False\n",
    ")\n",
    "\n",
    "print(f\"Loaded dataframe with shape {df.shape}\")\n",
    "\n",
    "for col in [\"content_date\", \"application_date\", \"created_at\"]:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_datetime(df[col], errors=\"coerce\", utc=True)\n",
    "\n",
    "# optimize categorical cols\n",
    "categorical_cols = [\n",
    "    \"decision_account\", \"decision_ground\",\n",
    "    \"incompatible_content_ground\", \"category\",\n",
    "    \"automated_detection\", \"automated_decision\",\n",
    "    \"platform_name\"\n",
    "]\n",
    "for col in categorical_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].astype(\"category\")\n",
    "\n",
    "print(\"Basic cleaning complete.\")\n",
    "\n",
    "output_path = \"../data/dsa_processed/discord_cleaned_subset.parquet\"\n",
    "df.to_parquet(output_path, index=False)\n",
    "print(f\"Cleaned + optimized dataset saved to {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
